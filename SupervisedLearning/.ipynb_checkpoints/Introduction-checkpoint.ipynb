{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8d7946-cf26-4baf-a143-000a5d4f8e4b",
   "metadata": {},
   "source": [
    "# Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07365eab-f501-465c-b8c0-5c9b5fe2a6fd",
   "metadata": {},
   "source": [
    "### First let's import a set with pandas to train our model \n",
    "\n",
    "This table has the fields **\"home, how_it_works, contact and bought\"** with 0's and 1's to refer to something that did or did not happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08c88003-62f0-4a51-9d28-3797d43561a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "uri = \"https://gist.githubusercontent.com/guilhermesilveira/2d2efa37d66b6c84a722ea627a897ced/raw/10968b997d885cbded1c92938c7a9912ba41c615/tracking.csv\"\n",
    "data = pd.read_csv(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeed920-2976-402f-95b2-55c2a83cbea4",
   "metadata": {},
   "source": [
    "## `Training and Test Models`\n",
    "\n",
    "When building a Machine Learning model, it is common to **separate the data into training and testing sets**. \n",
    "The *training set is used to train the model*, while the *test set is used to evaluate the model's performance* \n",
    "This is done to ensure that the model can generalize well to new data and not just memorize the training data. \n",
    "A real-world example is splitting an image dataset into training and testing and using the training set to \n",
    "train an image classification model and the test set to evaluate the model's accuracy. Here's an example of how this works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3ee6b3a-a04e-4211-9f26-30dcd3ce93a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "x = data[[\"home\", \"how_it_works\", \"contact\"]]\n",
    "y = data[[\"bought\"]]\n",
    "\n",
    "# Separating the train set and test set\n",
    "\n",
    "train_x = x[:75]\n",
    "train_y = y[:75]\n",
    "test_x = x[75:]\n",
    "test_y = y[75:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4ff5088-dcd1-48cc-bcd4-bc52a9fc46f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy was: 96.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivi/.local/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "# Training the model\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# Predictions\n",
    "\n",
    "predictions = model.predict(test_x)\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "accuracy = accuracy_score(test_y, predictions) * 100\n",
    "\n",
    "print(\"The accuracy was: %.2f%%\" %accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd526e4f-943b-472e-b7c4-ebba48b2d474",
   "metadata": {},
   "source": [
    "## `Layering Splits`\n",
    "\n",
    "Split Stratification **is a technique used in machine learning to divide a dataset into training and testing subsets** \n",
    "so that the distribution of classes is maintained in both subsets.\n",
    "\n",
    "In other words, splitting stratification ensures that the proportions of classes are the same in both sets, so as \n",
    "to prevent the trained model from being biased towards any particular class.\n",
    "\n",
    "#\n",
    "\n",
    "To perform splitting, you can use the function `train_test_split` from Python's `scikit-learn library`. This function \n",
    "can be used to split the dataset into training and test subsets randomly, but keeping the proportion of the classes. \n",
    "It always returns in this order (training X, test X, training Y, test Y).\n",
    "\n",
    "Here is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb63916d-ee78-4ab9-969f-0f9f401f3198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy was: 96.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivi/.local/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setting a random seed to avoid different \n",
    "# results each time you run the program.\n",
    "\n",
    "SEED = 20\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, random_state = SEED, test_size = 0.25)\n",
    "\n",
    "# Training now with the splitted trains\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# Predicting\n",
    "\n",
    "predictions = model.predict(test_x)\n",
    "\n",
    "accuracy = accuracy_score(test_y, predictions) * 100\n",
    "\n",
    "# Voi l√°, each time that you execute the program\n",
    "# will return the same result, in this case (96%)\n",
    "\n",
    "print(\"The accuracy was: %.2f%%\" %accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49413d4-9d61-4896-b0d2-8f45b6d6dbe0",
   "metadata": {},
   "source": [
    "## `Overfitting And Underfitting`\n",
    "\n",
    "This section will cover a more theoretical and fundamental part of machine learning, so I will not show \n",
    "exemplifying code, but I hope you understand the concepts.\n",
    "\n",
    "#\n",
    "\n",
    "### `Overfitting`\n",
    "\n",
    "Overfitting **occurs when the model is too complex relative to the training data**. In this case, the \n",
    "*model learns the details and noise from the training set, becoming too specific to the training data \n",
    "and thus unable to generalize to new data*. This can lead to poor model performance on test data. \n",
    "Overfitting can be identified by **a high performance on the training set, but a low performance on the test set**.\n",
    "\n",
    "#\n",
    "\n",
    "### `Underfitting`\n",
    "\n",
    "Underfitting, on the other hand, **occurs when the model is too simple to capture the nuances and \n",
    "complexities of the training data set**. In this case, *the model cannot fit the training data well \n",
    "and also cannot generalize well to new data.* This can lead to poor model performance on both training \n",
    "and test sets. Underfitting can be identified by **poor performance on both the training set and the test set**.\n",
    "\n",
    "#\n",
    "\n",
    "### `Resolution`\n",
    "\n",
    "To solve these problems, it is necessary to **adjust the complexity of the model**. If the model is \n",
    "suffering from `overfitting`, it is necessary to **reduce the complexity of the model**. For example, \n",
    "by **reducing the number of features, using regularization, or reducing the depth of the neural network**. \n",
    "But, if the model is suffering from `underfitting`, it is necessary to **increase the complexity of the model**. \n",
    "For example, by **adding more features, increasing the depth of the neural network, or changing the hyperparameters of the model.**\n",
    "\n",
    "\n",
    "Problem | Resolution\n",
    "------- | ----------\n",
    "Overfitting | Reduce the complexity of the model\n",
    "Underfitting | Increase the complexity of the model\n",
    "\n",
    "Evaluating the performance of the model on a test dataset is also important to identify overfitting \n",
    "and underfitting problems. This can be done using a variety of **evaluation metrics, such as accuracy, \n",
    "recall, F1-score, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca02182-6b71-4be2-98dc-8a536cafda75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
